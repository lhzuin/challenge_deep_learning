version_base: 1.1


defaults:
    - _self_
    - dataset: default
    - optim: adamw_class
    - model: siglip_distilbert_lora_class
    - loss_fn: cross_entropy

epochs: 2


early_stopping:
  patience: 2
  min_epochs: 7
use_warmup: true
warmup_fraction: 0.02
unfreeze_fraction: 0.15 #not used
layer_decay: ${if:${dataset.train_on_log}, 0.9, 0.8}
log: False
prefix: ""
experiment_name: ${prefix}${model.name}_${now:%Y-%m-%d_%H-%M-%S}


hydra:
  output_subdir: null
  run:
    dir: .

datamodule:
  _target_: data.datamodule.DataModule
  dataset_path: ${data_dir}
  train_transform: ${dataset.train_transform}
  test_transform: ${dataset.val_transform}
  batch_size: ${dataset.batch_size}
  num_workers: ${dataset.num_workers}
  metadata: ${dataset.metadata}
  val_ratio: ${dataset.val_ratio}
  seed: ${dataset.seed}
  train_on_log: ${dataset.train_on_log}
  augmentation: ${dataset.augmentation}

accum_steps: ${dataset.accum_steps}
data_dir: ${root_dir}/dataset/
root_dir:  ${hydra:runtime.cwd}
checkpoint_path: ${root_dir}/checkpoints/${experiment_name}.pt



