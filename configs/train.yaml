version_base: 1.1

defaults:
    - _self_
    - dataset: default
    - optim: adamw_log
    - model: resnet_distilbert_lora  #siglip_distilbert_lora #resnet_distilbert_lora #siglip_distilbert_lora #img12_distilbert_attention_lora #siglip_gemma_attention_lora
    - loss_fn: mse

epochs: 10 #50 / 35

early_stopping:
  patience: 2
  min_epochs: 7
use_warmup: true
warmup_fraction: 0.02
unfreeze_fraction: 0.15 #not used
layer_decay: 0.8
log: True
prefix: ""
experiment_name: ${prefix}${model.name}_${now:%Y-%m-%d_%H-%M-%S}


hydra:
  output_subdir: null
  run:
    dir: .

datamodule:
  _target_: data.datamodule.ConcatDataModule
  dataset_path: ${data_dir}
  train_transform: ${dataset.train_transform}
  test_transform: ${dataset.val_transform}
  batch_size: ${dataset.batch_size}
  num_workers: ${dataset.num_workers}
  metadata: ${dataset.metadata}

  val_ratio: ${dataset.val_ratio}
  seed: ${dataset.seed}
  train_on_log: ${dataset.train_on_log}
  augmentation: ${dataset.augmentation}
  num_epochs: ${epochs}
  planification: "same_champion" #"outliers"

data_dir: ${root_dir}/dataset/
root_dir:  ${hydra:runtime.cwd}
checkpoint_path: ${root_dir}/checkpoints/${experiment_name}.pt
sanity_check:
  enabled: false
  num_samples: 1000
  batch_size: 1
  num_workers: 16


