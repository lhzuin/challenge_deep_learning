version_base: 1.1

defaults:
    - _self_
    - dataset: default
    - optim: adamw
    - model: img12_distilbert_attention_lora
    - loss_fn: msle

epochs: 4 #50 / 35


early_stopping:
  patience: 2
  min_epochs: 7
use_warmup: true
warmup_fraction: 0.02
unfreeze_fraction: 0.15 #not used
layer_decay: ${if:${dataset.train_on_log}, 0.9, 0.8}
log: True
prefix: ""
experiment_name: ${prefix}${model.name}_${now:%Y-%m-%d_%H-%M-%S}


hydra:
  output_subdir: null
  run:
    dir: .

datamodule:
  _target_: data.datamodule.DataModule
  dataset_path: ${data_dir}
  train_transform: ${dataset.train_transform}
  test_transform: ${dataset.val_transform}
  batch_size: ${dataset.batch_size}
  num_workers: ${dataset.num_workers}
  metadata: ${dataset.metadata}
<<<<<<< HEAD
  val_ratio: ${dataset.val_ratio}
  seed: ${dataset.seed}
  train_on_log: ${dataset.train_on_log}
  augmentation: ${dataset.augmentation}

=======
  num_epochs: ${epochs}
>>>>>>> 414b9f8 (train for plaf=nification)

data_dir: ${root_dir}/dataset/
root_dir:  ${hydra:runtime.cwd}
checkpoint_path: ${root_dir}/checkpoints/${experiment_name}.pt
sanity_check:
  enabled: false
  num_samples: 1000
  batch_size: 1
  num_workers: 16


