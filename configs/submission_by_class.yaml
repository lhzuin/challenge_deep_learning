defaults:
    - _self_
    - dataset: default
    - model: siglip_distilbert_lora

hydra:
  output_subdir: null
  run:
    dir: .

data:
  a: 1
  dataset_path: ${data_dir}
  test_transform: ${dataset.val_transform}
  batch_size: ${dataset.batch_size}
  num_workers: ${dataset.num_workers}
  metadata: ${dataset.metadata}

data_dir: ${root_dir}/dataset/
prefix: ""
time_stamp: 2025-05-29_13-28-20

experiment_name: ${prefix}${model.name}_${time_stamp}
root_dir:  ${hydra:runtime.cwd}
checkpoint_path: ${root_dir}/checkpoints/${experiment_name}.pt
unfreeze_fraction: 0.15
epochs: 90
train_on_log: ${dataset.train_on_log}
# paths to your three per-class checkpoints
ckpt_dir: checkpoints
# classifier checkpoint (outputs class probabilities)
ckpt_class: checkpoints/SIGLIP_DISTILBERT_LORA_CLASS_final.pt
classif:
  _target_: models.siglip_distilbert_lora_class.SigLIPDistilBertLoraClass
  num_channels: 47
  head_hidden_dim: 256
  head_dropout: 0.2
  metadata_dropout: 0.2
  proj_dropout: 0.10
  ch_emb_dim: 16
  year_proj_dim: 8
  cy_hidden: 16


datamodule:
  _target_: data.datamodule.DataModule
  dataset_path: ${data_dir}
  train_transform: ${dataset.train_transform}
  test_transform: ${dataset.val_transform}
  batch_size: ${dataset.batch_size}
  num_workers: ${dataset.num_workers}
  metadata: ${dataset.metadata}
  val_ratio: ${dataset.val_ratio}
  seed: ${dataset.seed}
  train_on_log: ${dataset.train_on_log}
  augmentation: ${dataset.augmentation}