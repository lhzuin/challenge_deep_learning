defaults:
  - _self_
pretrained: webli
instance:
  _target_: models.siglip_gemma_attention_lora_log.SigLIPGemmaAttentionLoraLog
  num_channels: 47
  head_hidden_dim: 256
  head_dropout: ${if:${dataset.train_on_log}, 0.2, 0.15}
  metadata_dropout: 0.3
  ch_emb_dim: 16
  year_proj_dim: 8
  date_proj_dim: 16
  cy_hidden: 16

name: SIGLIP_GEMMA_ATTENTION_LORA_LOG